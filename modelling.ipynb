{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/davide/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import softmax, relu\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "debug = False\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39798, 10168)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_hdf(\"data/train.hdf5\")\n",
    "df_test = pd.read_hdf(\"data/test.hdf5\")\n",
    "\n",
    "len(df_train), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(no    14979\n",
       " da    14305\n",
       " sv    10514\n",
       " Name: label, dtype: int64,\n",
       " no    3887\n",
       " da    3637\n",
       " sv    2644\n",
       " Name: label, dtype: int64)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if debug:\n",
    "    df_train = shuffle_df(df_train)\n",
    "    df_test = shuffle_df(df_test)\n",
    "    df_train = df_train.iloc[:int(len(df_train)/ 4)]\n",
    "    df_test = df_test.iloc[:int(len(df_train)/ 4)]\n",
    "    \n",
    "df_train[\"label\"].value_counts(), df_test[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(line):\n",
    "    words = nltk.word_tokenize(line)\n",
    "    tokens = [word for word in words if word.isalnum()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"data\"] = df_train[\"data\"].apply(tokenize)\n",
    "df_test[\"data\"] = df_test[\"data\"].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df_train[\"label\"].values)\n",
    "le.classes_\n",
    "df_train[\"y\"] = le.transform(df_train[\"label\"])\n",
    "df_test[\"y\"] = le.transform(df_test[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "vocab = set()\n",
    "for line in df_train[\"data\"].values:\n",
    "    vocab.update(set(line))\n",
    "for line in df_test[\"data\"].values:\n",
    "    vocab.update(set(line))\n",
    "\n",
    "# Build a word to index lookup\n",
    "w2i = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109018"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (embeddings): Embedding(109018, 32)\n",
      "  (rnn_1): LSTM(32, 100, num_layers=2, bidirectional=True)\n",
      "  (l_out): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=200, bias=True)\n",
      "    (1): Dropout(p=0.2, inplace=False)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=200, out_features=64, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Linear(in_features=64, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.embeddings = nn.Embedding(len(w2i), 32)\n",
    "        \n",
    "        self.rnn_1 = nn.LSTM(input_size=32,\n",
    "                         hidden_size=100,\n",
    "                         num_layers=2,\n",
    "                         bidirectional=True,\n",
    "                         batch_first=False)\n",
    "        \n",
    "        self.l_out = nn.Sequential(\n",
    "            nn.Linear(400, 200),\n",
    "            nn.Dropout(.2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(200, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(64, 3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = {}\n",
    "        # get embeddings\n",
    "        x = self.embeddings(x)\n",
    "        \n",
    "        # output, hidden state\n",
    "        x, hn = self.rnn_1(x)\n",
    "        \n",
    "        out['hidden'] = x = torch.cat((torch.mean(x, dim=0), torch.max(x, dim=0)[0]), dim=1)\n",
    "        \n",
    "        # classify\n",
    "        out['out'] = softmax(self.l_out(x), dim=1)\n",
    "        return out\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_batch(df_batch):\n",
    "    # Get indices\n",
    "    inputs = [[w2i[token] for token in row] for y, row in df_batch[\"data\"].iteritems()]\n",
    "    \n",
    "    # Get the longest row\n",
    "    longest = max([len(row) for row in inputs])\n",
    "\n",
    "    # Make the rows equal size\n",
    "    new_inputs = np.empty([len(df_batch), longest])\n",
    "    for i in range(len(df_batch)):\n",
    "        if len(inputs[i]) == 0:        \n",
    "            new_inputs[i] = np.pad(inputs[i], (0, longest - len(inputs[i])), 'constant', constant_values=0)\n",
    "        else:\n",
    "            new_inputs[i] = np.pad(inputs[i], (0, longest - len(inputs[i])), 'wrap')\n",
    "\n",
    "    inp = torch.Tensor(new_inputs.T).long()\n",
    "    \n",
    "    return inp\n",
    "\n",
    "# Shuffle the rows of a pandas data frame\n",
    "def shuffle_df(df):\n",
    "    return df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Return an iterable over mini-batches\n",
    "def batchify(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33723446105428795"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(df_test[\"y\"], np.random.randint(3, size=len(df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, df_test, batch_size, epoch):\n",
    "    batches = batchify(df_test, batch_size)\n",
    "    net.eval()\n",
    "    loss = []\n",
    "    out = []\n",
    "    for df_batch in batches:\n",
    "        inp = create_input_batch(df_batch)\n",
    "        labels = torch.Tensor(df_batch['y'].values).long()\n",
    "        output = net(inp)\n",
    "        batch_loss = criterion(output['out'], labels)\n",
    "        loss.append(batch_loss.item())\n",
    "        _, pred = torch.max(output['out'].detach(), 1)\n",
    "        out.extend(pred)\n",
    "        \n",
    "    mean_loss = np.mean(loss)\n",
    "    accuracy = accuracy_score(df_test['y'], out)\n",
    "        \n",
    "    print(f\"Validation loss after {epoch} epoch: {mean_loss}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    \n",
    "    return mean_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(net, experiment, accuracy):\n",
    "    model_path = os.path.join(\"data\", \"models\", experiment)\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "        \n",
    "    path = os.path.join(model_path, f\"{accuracy}_.pt\")\n",
    "    \n",
    "    current_best = list(filter(lambda x: x.endswith(\".pt\"), os.listdir(model_path)))\n",
    "    if len(current_best) == 0:\n",
    "        torch.save(net.state_dict(), path)\n",
    "        return True\n",
    "        \n",
    "    current_best_acc = float(current_best[0].split(\"_\")[0])\n",
    "    if accuracy > current_best_acc:\n",
    "        torch.save(net.state_dict(), path)\n",
    "        os.remove(os.path.join(model_path, current_best[0]))\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Validation loss after 0 epoch: 1.0959775428195575\n",
      "Accuracy: 0.35769079464988196\n",
      "39798\n",
      "Iteration:0/711 loss: 1.1030023097991943\n",
      "Iteration:60/711 loss: 0.8589862585067749\n",
      "Iteration:120/711 loss: 0.8638518452644348\n",
      "Iteration:180/711 loss: 0.8414367437362671\n",
      "Iteration:240/711 loss: 0.7121663093566895\n",
      "Iteration:300/711 loss: 0.7795535326004028\n",
      "Iteration:360/711 loss: 0.790208637714386\n",
      "Iteration:420/711 loss: 0.7073476910591125\n",
      "Iteration:480/711 loss: 0.7201284170150757\n",
      "Iteration:540/711 loss: 0.7387074828147888\n",
      "Iteration:600/711 loss: 0.6602550148963928\n",
      "Iteration:660/711 loss: 0.6929439306259155\n",
      "Validation loss after 0 epoch: 0.7344094468997076\n",
      "Accuracy: 0.8078284815106216\n",
      "New best model saved.\n",
      "Epoch: 1\n",
      "39798\n",
      "Iteration:0/711 loss: 0.6937600374221802\n",
      "Iteration:60/711 loss: 0.6893138289451599\n",
      "Iteration:120/711 loss: 0.6696398854255676\n",
      "Iteration:180/711 loss: 0.6806906461715698\n",
      "Iteration:240/711 loss: 0.692054808139801\n",
      "Iteration:300/711 loss: 0.6445044279098511\n",
      "Iteration:360/711 loss: 0.6586939096450806\n",
      "Iteration:420/711 loss: 0.7049582600593567\n",
      "Iteration:480/711 loss: 0.7028629183769226\n",
      "Iteration:540/711 loss: 0.6504389047622681\n",
      "Iteration:600/711 loss: 0.6351618766784668\n",
      "Iteration:660/711 loss: 0.7203648686408997\n",
      "Validation loss after 1 epoch: 0.694352476806431\n",
      "Accuracy: 0.8516915814319433\n",
      "New best model saved.\n",
      "Epoch: 2\n",
      "39798\n",
      "Iteration:0/711 loss: 0.604566216468811\n",
      "Iteration:60/711 loss: 0.64791339635849\n",
      "Iteration:120/711 loss: 0.6942386627197266\n",
      "Iteration:180/711 loss: 0.6433767676353455\n",
      "Iteration:240/711 loss: 0.6140061020851135\n",
      "Iteration:300/711 loss: 0.6231951117515564\n",
      "Iteration:360/711 loss: 0.621452271938324\n",
      "Iteration:420/711 loss: 0.5953541398048401\n",
      "Iteration:480/711 loss: 0.666667640209198\n",
      "Iteration:540/711 loss: 0.6687939763069153\n",
      "Iteration:600/711 loss: 0.643524706363678\n",
      "Iteration:660/711 loss: 0.6604346036911011\n",
      "Validation loss after 2 epoch: 0.679274680522772\n",
      "Accuracy: 0.8678206136900078\n",
      "New best model saved.\n",
      "Epoch: 3\n",
      "39798\n",
      "Iteration:0/711 loss: 0.5871405601501465\n",
      "Iteration:60/711 loss: 0.6419889330863953\n",
      "Iteration:120/711 loss: 0.6437426805496216\n",
      "Iteration:180/711 loss: 0.6322230696678162\n",
      "Iteration:240/711 loss: 0.7032257914543152\n",
      "Iteration:300/711 loss: 0.6569794416427612\n",
      "Iteration:360/711 loss: 0.6130930781364441\n",
      "Iteration:420/711 loss: 0.693992555141449\n",
      "Iteration:480/711 loss: 0.6491988897323608\n",
      "Iteration:540/711 loss: 0.6259395480155945\n",
      "Iteration:600/711 loss: 0.6819856762886047\n",
      "Iteration:660/711 loss: 0.6205143928527832\n",
      "Validation loss after 3 epoch: 0.6721861542581202\n",
      "Accuracy: 0.8762785208497246\n",
      "New best model saved.\n",
      "Epoch: 4\n",
      "39798\n",
      "Iteration:0/711 loss: 0.5701972246170044\n",
      "Iteration:60/711 loss: 0.5700110793113708\n",
      "Iteration:120/711 loss: 0.6096552014350891\n",
      "Iteration:180/711 loss: 0.5975362062454224\n",
      "Iteration:240/711 loss: 0.6675189137458801\n",
      "Iteration:300/711 loss: 0.6446712613105774\n",
      "Iteration:360/711 loss: 0.6223984360694885\n",
      "Iteration:420/711 loss: 0.6092914342880249\n",
      "Iteration:480/711 loss: 0.6959436535835266\n",
      "Iteration:540/711 loss: 0.590795636177063\n",
      "Iteration:600/711 loss: 0.6911789774894714\n",
      "Iteration:660/711 loss: 0.5705495476722717\n",
      "Validation loss after 4 epoch: 0.6749324356461619\n",
      "Accuracy: 0.8657553107789142\n",
      "New best model saved.\n",
      "Epoch: 5\n",
      "39798\n",
      "Iteration:0/711 loss: 0.6095911264419556\n",
      "Iteration:60/711 loss: 0.6406474709510803\n",
      "Iteration:120/711 loss: 0.581516683101654\n",
      "Iteration:180/711 loss: 0.634206235408783\n",
      "Iteration:240/711 loss: 0.563908576965332\n",
      "Iteration:300/711 loss: 0.635582447052002\n",
      "Iteration:360/711 loss: 0.6473870873451233\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-8148ee08df55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m60\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lan_wire/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lan_wire/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 56\n",
    "batches = batchify(df_train, batch_size)\n",
    "length = sum(1 for x in batches)\n",
    "experiment = \"LSTM\"\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    if epoch == 0:\n",
    "        validate(net, df_test, batch_size, epoch)\n",
    "    net.train()\n",
    "    shuffled_df = shuffle_df(df_train)\n",
    "    print(len(shuffled_df))\n",
    "    counter = 0\n",
    "    batches = batchify(shuffled_df, batch_size)\n",
    "    \n",
    "    for df_batch in batches:\n",
    "        \n",
    "        inp = create_input_batch(df_batch)\n",
    "        labels = torch.Tensor(df_batch['y'].values).long()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = net(inp)\n",
    "\n",
    "        batch_loss = criterion(output['out'], labels)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        if(counter % 60 == 0):\n",
    "            print(f\"Iteration:{counter}/{length} loss: {batch_loss.item()}\")\n",
    "            \n",
    "        \n",
    "        counter += 1\n",
    "    _, accuracy = validate(net, df_test, batch_size, epoch)\n",
    "    saved = save_best_model(net, experiment, accuracy)\n",
    "    if saved:\n",
    "        print(f\"New best model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[\"y\"] for _, entry in batch.iterrows()])\n",
    "    text = [entry[\"data\"] for _, entry in batch.iterrows()]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    # torch.Tensor.cumsum returns the cumulative sum\n",
    "    # of elements in the dimension dim.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_train.iloc[:3][\"data\"].values\n",
    "y = df_train.iloc[:3][\"y\"].values\n",
    "\n",
    "batch = [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-e7358b11ec6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-80-d5ef3a0aeeee>\u001b[0m in \u001b[0;36mgenerate_batch\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moffsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
     ]
    }
   ],
   "source": [
    "generate_batch(df_train.iloc[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[politik, i, se, også, kategori, begivenheder, i]</td>\n",
       "      <td>da</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[politik, i, se, også, kategori, begivenheder, i]</td>\n",
       "      <td>da</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[organisationen, til, oplysning, om, atomkraft...</td>\n",
       "      <td>da</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data label  y\n",
       "1  [politik, i, se, også, kategori, begivenheder, i]    da  0\n",
       "2  [politik, i, se, også, kategori, begivenheder, i]    da  0\n",
       "3  [organisationen, til, oplysning, om, atomkraft...    da  0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/davide/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import softmax, relu\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "debug = False\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39798, 10168)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_hdf(\"app/lang_model/data/train.hdf5\")\n",
    "df_test = pd.read_hdf(\"app/lang_model/data/test.hdf5\")\n",
    "\n",
    "len(df_train), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(no    14979\n",
       " da    14305\n",
       " sv    10514\n",
       " Name: label, dtype: int64,\n",
       " no    3887\n",
       " da    3637\n",
       " sv    2644\n",
       " Name: label, dtype: int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if debug:\n",
    "    df_train = shuffle_df(df_train)\n",
    "    df_test = shuffle_df(df_test)\n",
    "    df_train = df_train.iloc[:int(len(df_train)/ 4)]\n",
    "    df_test = df_test.iloc[:int(len(df_train)/ 4)]\n",
    "    \n",
    "df_train[\"label\"].value_counts(), df_test[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(line):\n",
    "    words = nltk.word_tokenize(line)\n",
    "    tokens = [word for word in words if word.isalnum()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"data\"] = df_train[\"data\"].apply(tokenize)\n",
    "df_test[\"data\"] = df_test[\"data\"].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['da' 'no' 'sv']\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df_train[\"label\"].values)\n",
    "print(le.classes_)\n",
    "df_train[\"y\"] = le.transform(df_train[\"label\"])\n",
    "df_test[\"y\"] = le.transform(df_test[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[grønlands, politik]</td>\n",
       "      <td>da</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[ooa, var, en, åben, bevægelse, med, et, lands...</td>\n",
       "      <td>da</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[i, august, samlede, to, store, atomkraftmarch...</td>\n",
       "      <td>da</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[den]</td>\n",
       "      <td>da</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[november, begyndte, oaa, som, skulle, vise, a...</td>\n",
       "      <td>da</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49944</th>\n",
       "      <td>[i, en, del, tilfeller, ble, det, oppnådd, ell...</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49945</th>\n",
       "      <td>[prosessen, for, sosiale, og, politiske, endri...</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49948</th>\n",
       "      <td>[begrepet, henspiller, ikke, på, en, bestemt, ...</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49949</th>\n",
       "      <td>[den]</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49951</th>\n",
       "      <td>[januar, da, alexander, dubček, kom, til, makt...</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10168 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    data label  y\n",
       "0                                   [grønlands, politik]    da  0\n",
       "7      [ooa, var, en, åben, bevægelse, med, et, lands...    da  0\n",
       "11     [i, august, samlede, to, store, atomkraftmarch...    da  0\n",
       "15                                                 [den]    da  0\n",
       "19     [november, begyndte, oaa, som, skulle, vise, a...    da  0\n",
       "...                                                  ...   ... ..\n",
       "49944  [i, en, del, tilfeller, ble, det, oppnådd, ell...    no  1\n",
       "49945  [prosessen, for, sosiale, og, politiske, endri...    no  1\n",
       "49948  [begrepet, henspiller, ikke, på, en, bestemt, ...    no  1\n",
       "49949                                              [den]    no  1\n",
       "49951  [januar, da, alexander, dubček, kom, til, makt...    no  1\n",
       "\n",
       "[10168 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "vocab = set()\n",
    "for line in df_train[\"data\"].values:\n",
    "    vocab.update(set(line))\n",
    "for line in df_test[\"data\"].values:\n",
    "    vocab.update(set(line))\n",
    "\n",
    "# Build a word to index lookup\n",
    "w2i = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109018"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangModel(\n",
      "  (embeddings): Embedding(109018, 32)\n",
      "  (rnn_1): LSTM(32, 100, num_layers=2, bidirectional=True)\n",
      "  (l_out): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=200, bias=True)\n",
      "    (1): Dropout(p=0.2, inplace=False)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=200, out_features=64, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Linear(in_features=64, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LangModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(LangModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, 32)\n",
    "\n",
    "        self.rnn_1 = nn.LSTM(\n",
    "            input_size=32,\n",
    "            hidden_size=100,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=False,\n",
    "        )\n",
    "\n",
    "        self.l_out = nn.Sequential(\n",
    "            nn.Linear(400, 200),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(200, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(64, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = {}\n",
    "        # get embeddings\n",
    "        x = self.embeddings(x)\n",
    "\n",
    "        # output, hidden state\n",
    "        x, _ = self.rnn_1(x)\n",
    "\n",
    "        x = torch.cat((torch.mean(x, dim=0), torch.max(x, dim=0)[0]), dim=1)\n",
    "\n",
    "        # classify\n",
    "        out[\"out\"] = softmax(self.l_out(x), dim=1)\n",
    "        return out\n",
    "\n",
    "net = LangModel(len(w2i)).to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_batch(df_batch):\n",
    "    # Get indices\n",
    "    inputs = [[w2i[token] for token in row] for y, row in df_batch[\"data\"].iteritems()]\n",
    "    \n",
    "    # Get the longest row\n",
    "    longest = max([len(row) for row in inputs])\n",
    "\n",
    "    # Make the rows equal size\n",
    "    new_inputs = np.empty([len(df_batch), longest])\n",
    "    for i in range(len(df_batch)):\n",
    "        if len(inputs[i]) == 0:        \n",
    "            new_inputs[i] = np.pad(inputs[i], (0, longest - len(inputs[i])), 'constant', constant_values=0)\n",
    "        else:\n",
    "            new_inputs[i] = np.pad(inputs[i], (0, longest - len(inputs[i])), 'wrap')\n",
    "\n",
    "    inp = torch.Tensor(new_inputs.T).to(device).long()\n",
    "    \n",
    "    return inp\n",
    "\n",
    "# Shuffle the rows of a pandas data frame\n",
    "def shuffle_df(df):\n",
    "    return df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Return an iterable over mini-batches\n",
    "def batchify(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33477576711250984"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(df_test[\"y\"], np.random.randint(3, size=len(df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, df_test, batch_size, epoch):\n",
    "    batches = batchify(df_test, batch_size)\n",
    "    net.eval()\n",
    "    loss = []\n",
    "    out = []\n",
    "    for df_batch in batches:\n",
    "        inp = create_input_batch(df_batch)\n",
    "        labels = torch.Tensor(df_batch['y'].values).to(device).long()\n",
    "        output = net(inp)\n",
    "        batch_loss = criterion(output['out'], labels)\n",
    "        loss.append(batch_loss.item())\n",
    "        _, pred = torch.max(output['out'].detach(), 1)\n",
    "        out.extend(pred)\n",
    "        \n",
    "    mean_loss = np.mean(loss)\n",
    "    accuracy = accuracy_score(df_test['y'], out)\n",
    "        \n",
    "    print(f\"Validation loss after {epoch} epoch: {mean_loss}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    \n",
    "    return mean_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(net, experiment, accuracy):\n",
    "    model_path = os.path.join(\"app\", \"lang_model\", \"data\", \"models\", experiment)\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "        \n",
    "    path = os.path.join(model_path, f\"{accuracy}_.pt\")\n",
    "    \n",
    "    current_best = list(filter(lambda x: x.endswith(\".pt\"), os.listdir(model_path)))\n",
    "    if len(current_best) == 0:\n",
    "        torch.save(net.state_dict(), path)\n",
    "        return True\n",
    "        \n",
    "    current_best_acc = float(current_best[0].split(\"_\")[0])\n",
    "    if accuracy > current_best_acc:\n",
    "        torch.save(net.state_dict(), path)\n",
    "        os.remove(os.path.join(model_path, current_best[0]))\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Validation loss after 0 epoch: 0.8718964794834891\n",
      "Accuracy: 0.6791896144767899\n",
      "39798\n",
      "Iteration:0/711 loss: 0.9078559279441833\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-da6dd262a3ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lan_wire/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-d3c903a2240e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# output, hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lan_wire/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lan_wire/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 582\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    583\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 56\n",
    "batches = batchify(df_train, batch_size)\n",
    "length = sum(1 for x in batches)\n",
    "experiment = \"LSTM\"\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    if epoch == 0:\n",
    "        validate(net, df_test, batch_size, epoch)\n",
    "    net.train()\n",
    "    shuffled_df = shuffle_df(df_train)\n",
    "    print(len(shuffled_df))\n",
    "    counter = 0\n",
    "    batches = batchify(shuffled_df, batch_size)\n",
    "    \n",
    "    for df_batch in batches:\n",
    "        \n",
    "        inp = create_input_batch(df_batch)\n",
    "        labels = torch.Tensor(df_batch['y'].values).to(device).long()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = net(inp)\n",
    "\n",
    "        batch_loss = criterion(output['out'], labels)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        if(counter % 60 == 0):\n",
    "            print(f\"Iteration:{counter}/{length} loss: {batch_loss.item()}\")\n",
    "            \n",
    "        \n",
    "        counter += 1\n",
    "    _, accuracy = validate(net, df_test, batch_size, epoch)\n",
    "    saved = save_best_model(net, experiment, accuracy)\n",
    "    if saved:\n",
    "        print(f\"New best model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('app/lang_model/data/vocab/vocab.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(vocab, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

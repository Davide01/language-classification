{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/davide/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import softmax, relu\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "debug = False\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39798, 10168)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_hdf(\"data/train.hdf5\")\n",
    "df_test = pd.read_hdf(\"data/test.hdf5\")\n",
    "\n",
    "len(df_train), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(no    14979\n",
       " da    14305\n",
       " sv    10514\n",
       " Name: label, dtype: int64,\n",
       " no    3887\n",
       " da    3637\n",
       " sv    2644\n",
       " Name: label, dtype: int64)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if debug:\n",
    "    df_train = shuffle_df(df_train)\n",
    "    df_test = shuffle_df(df_test)\n",
    "    df_train = df_train.iloc[:int(len(df_train)/ 4)]\n",
    "    df_test = df_test.iloc[:int(len(df_train)/ 4)]\n",
    "    \n",
    "df_train[\"label\"].value_counts(), df_test[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(line):\n",
    "    words = nltk.word_tokenize(line)\n",
    "    tokens = [word for word in words if word.isalnum()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"data\"] = df_train[\"data\"].apply(tokenize)\n",
    "df_test[\"data\"] = df_test[\"data\"].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['da' 'no' 'sv']\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df_train[\"label\"].values)\n",
    "print(le.classes_)\n",
    "df_train[\"y\"] = le.transform(df_train[\"label\"])\n",
    "df_test[\"y\"] = le.transform(df_test[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[grønlands, politik]</td>\n",
       "      <td>da</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[ooa, var, en, åben, bevægelse, med, et, lands...</td>\n",
       "      <td>da</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[i, august, samlede, to, store, atomkraftmarch...</td>\n",
       "      <td>da</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[den]</td>\n",
       "      <td>da</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[november, begyndte, oaa, som, skulle, vise, a...</td>\n",
       "      <td>da</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49944</th>\n",
       "      <td>[i, en, del, tilfeller, ble, det, oppnådd, ell...</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49945</th>\n",
       "      <td>[prosessen, for, sosiale, og, politiske, endri...</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49948</th>\n",
       "      <td>[begrepet, henspiller, ikke, på, en, bestemt, ...</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49949</th>\n",
       "      <td>[den]</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49951</th>\n",
       "      <td>[januar, da, alexander, dubček, kom, til, makt...</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10168 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    data label  y\n",
       "0                                   [grønlands, politik]    da  0\n",
       "7      [ooa, var, en, åben, bevægelse, med, et, lands...    da  0\n",
       "11     [i, august, samlede, to, store, atomkraftmarch...    da  0\n",
       "15                                                 [den]    da  0\n",
       "19     [november, begyndte, oaa, som, skulle, vise, a...    da  0\n",
       "...                                                  ...   ... ..\n",
       "49944  [i, en, del, tilfeller, ble, det, oppnådd, ell...    no  1\n",
       "49945  [prosessen, for, sosiale, og, politiske, endri...    no  1\n",
       "49948  [begrepet, henspiller, ikke, på, en, bestemt, ...    no  1\n",
       "49949                                              [den]    no  1\n",
       "49951  [januar, da, alexander, dubček, kom, til, makt...    no  1\n",
       "\n",
       "[10168 rows x 3 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "vocab = set()\n",
    "for line in df_train[\"data\"].values:\n",
    "    vocab.update(set(line))\n",
    "for line in df_test[\"data\"].values:\n",
    "    vocab.update(set(line))\n",
    "\n",
    "# Build a word to index lookup\n",
    "w2i = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109018"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangModel(\n",
      "  (embeddings): Embedding(109018, 32)\n",
      "  (rnn_1): LSTM(32, 100, num_layers=2, bidirectional=True)\n",
      "  (l_out): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=200, bias=True)\n",
      "    (1): Dropout(p=0.2, inplace=False)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=200, out_features=64, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Linear(in_features=64, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LangModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(LangModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, 32)\n",
    "\n",
    "        self.rnn_1 = nn.LSTM(\n",
    "            input_size=32,\n",
    "            hidden_size=100,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=False,\n",
    "        )\n",
    "\n",
    "        self.l_out = nn.Sequential(\n",
    "            nn.Linear(400, 200),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(200, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(64, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = {}\n",
    "        # get embeddings\n",
    "        x = self.embeddings(x)\n",
    "\n",
    "        # output, hidden state\n",
    "        x, _ = self.rnn_1(x)\n",
    "\n",
    "        x = torch.cat((torch.mean(x, dim=0), torch.max(x, dim=0)[0]), dim=1)\n",
    "\n",
    "        # classify\n",
    "        out[\"out\"] = softmax(self.l_out(x), dim=1)\n",
    "        return out\n",
    "\n",
    "net = LangModel(len(w2i))\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_batch(df_batch):\n",
    "    # Get indices\n",
    "    inputs = [[w2i[token] for token in row] for y, row in df_batch[\"data\"].iteritems()]\n",
    "    \n",
    "    # Get the longest row\n",
    "    longest = max([len(row) for row in inputs])\n",
    "\n",
    "    # Make the rows equal size\n",
    "    new_inputs = np.empty([len(df_batch), longest])\n",
    "    for i in range(len(df_batch)):\n",
    "        if len(inputs[i]) == 0:        \n",
    "            new_inputs[i] = np.pad(inputs[i], (0, longest - len(inputs[i])), 'constant', constant_values=0)\n",
    "        else:\n",
    "            new_inputs[i] = np.pad(inputs[i], (0, longest - len(inputs[i])), 'wrap')\n",
    "\n",
    "    inp = torch.Tensor(new_inputs.T).long()\n",
    "    \n",
    "    return inp\n",
    "\n",
    "# Shuffle the rows of a pandas data frame\n",
    "def shuffle_df(df):\n",
    "    return df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Return an iterable over mini-batches\n",
    "def batchify(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3268095987411487"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(df_test[\"y\"], np.random.randint(3, size=len(df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, df_test, batch_size, epoch):\n",
    "    batches = batchify(df_test, batch_size)\n",
    "    net.eval()\n",
    "    loss = []\n",
    "    out = []\n",
    "    for df_batch in batches:\n",
    "        inp = create_input_batch(df_batch)\n",
    "        labels = torch.Tensor(df_batch['y'].values).long()\n",
    "        output = net(inp)\n",
    "        batch_loss = criterion(output['out'], labels)\n",
    "        loss.append(batch_loss.item())\n",
    "        _, pred = torch.max(output['out'].detach(), 1)\n",
    "        out.extend(pred)\n",
    "        \n",
    "    mean_loss = np.mean(loss)\n",
    "    accuracy = accuracy_score(df_test['y'], out)\n",
    "        \n",
    "    print(f\"Validation loss after {epoch} epoch: {mean_loss}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    \n",
    "    return mean_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(net, experiment, accuracy):\n",
    "    model_path = os.path.join(\"app\", \"lang_model\", \"data\", \"models\", experiment)\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "        \n",
    "    path = os.path.join(model_path, f\"{accuracy}_.pt\")\n",
    "    \n",
    "    current_best = list(filter(lambda x: x.endswith(\".pt\"), os.listdir(model_path)))\n",
    "    if len(current_best) == 0:\n",
    "        torch.save(net.state_dict(), path)\n",
    "        return True\n",
    "        \n",
    "    current_best_acc = float(current_best[0].split(\"_\")[0])\n",
    "    if accuracy > current_best_acc:\n",
    "        torch.save(net.state_dict(), path)\n",
    "        os.remove(os.path.join(model_path, current_best[0]))\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Validation loss after 0 epoch: 0.6835297775137555\n",
      "Accuracy: 0.8654602675059009\n",
      "39798\n",
      "Iteration:0/711 loss: 0.6724043488502502\n",
      "Iteration:60/711 loss: 0.6087504029273987\n",
      "Iteration:120/711 loss: 0.6661273241043091\n",
      "Iteration:180/711 loss: 0.5763556361198425\n",
      "Iteration:240/711 loss: 0.5621869564056396\n",
      "Iteration:300/711 loss: 0.6346812844276428\n",
      "Iteration:360/711 loss: 0.6521762609481812\n",
      "Iteration:420/711 loss: 0.6059050559997559\n",
      "Iteration:480/711 loss: 0.6044569611549377\n",
      "Iteration:540/711 loss: 0.6396341323852539\n",
      "Iteration:600/711 loss: 0.5786057114601135\n",
      "Iteration:660/711 loss: 0.5724177360534668\n",
      "Validation loss after 0 epoch: 0.6752044149152525\n",
      "Accuracy: 0.872639653815893\n",
      "Epoch: 1\n",
      "39798\n",
      "Iteration:0/711 loss: 0.5964111685752869\n",
      "Iteration:60/711 loss: 0.6215572953224182\n",
      "Iteration:120/711 loss: 0.5518143773078918\n",
      "Iteration:180/711 loss: 0.5887036323547363\n",
      "Iteration:240/711 loss: 0.6631374955177307\n",
      "Iteration:300/711 loss: 0.6018176078796387\n",
      "Iteration:360/711 loss: 0.6067214012145996\n",
      "Iteration:420/711 loss: 0.6624981760978699\n",
      "Iteration:480/711 loss: 0.6174734830856323\n",
      "Iteration:540/711 loss: 0.6221230626106262\n",
      "Iteration:600/711 loss: 0.6041107773780823\n",
      "Iteration:660/711 loss: 0.6769575476646423\n",
      "Validation loss after 1 epoch: 0.6731313935347966\n",
      "Accuracy: 0.8753933910306845\n",
      "Epoch: 2\n",
      "39798\n",
      "Iteration:0/711 loss: 0.6942101120948792\n",
      "Iteration:60/711 loss: 0.6399306654930115\n",
      "Iteration:120/711 loss: 0.6349678635597229\n",
      "Iteration:180/711 loss: 0.5697957873344421\n",
      "Iteration:240/711 loss: 0.58881014585495\n",
      "Iteration:300/711 loss: 0.6197137236595154\n",
      "Iteration:360/711 loss: 0.592247486114502\n",
      "Iteration:420/711 loss: 0.6055608987808228\n",
      "Iteration:480/711 loss: 0.6131259799003601\n",
      "Iteration:540/711 loss: 0.5781766176223755\n",
      "Iteration:600/711 loss: 0.6452285647392273\n",
      "Iteration:660/711 loss: 0.6845487356185913\n",
      "Validation loss after 2 epoch: 0.6711692767483848\n",
      "Accuracy: 0.8780487804878049\n",
      "Epoch: 3\n",
      "39798\n",
      "Iteration:0/711 loss: 0.6052038073539734\n",
      "Iteration:60/711 loss: 0.5872134566307068\n",
      "Iteration:120/711 loss: 0.5878519415855408\n",
      "Iteration:180/711 loss: 0.5696487426757812\n",
      "Iteration:240/711 loss: 0.5765619277954102\n",
      "Iteration:300/711 loss: 0.6172770261764526\n",
      "Iteration:360/711 loss: 0.6121104955673218\n",
      "Iteration:420/711 loss: 0.5769725441932678\n",
      "Iteration:480/711 loss: 0.6476219296455383\n",
      "Iteration:540/711 loss: 0.5672754049301147\n",
      "Iteration:600/711 loss: 0.5872858166694641\n",
      "Iteration:660/711 loss: 0.5842784643173218\n",
      "Validation loss after 3 epoch: 0.6719074612790412\n",
      "Accuracy: 0.8780487804878049\n",
      "Epoch: 4\n",
      "39798\n",
      "Iteration:0/711 loss: 0.587426483631134\n",
      "Iteration:60/711 loss: 0.578105628490448\n",
      "Iteration:120/711 loss: 0.552013099193573\n",
      "Iteration:180/711 loss: 0.6230830550193787\n",
      "Iteration:240/711 loss: 0.55158531665802\n",
      "Iteration:300/711 loss: 0.5853527784347534\n",
      "Iteration:360/711 loss: 0.6360525488853455\n",
      "Iteration:420/711 loss: 0.5854921340942383\n",
      "Iteration:480/711 loss: 0.6051657795906067\n",
      "Iteration:540/711 loss: 0.6051357984542847\n",
      "Iteration:600/711 loss: 0.625626266002655\n",
      "Iteration:660/711 loss: 0.5711883306503296\n",
      "Validation loss after 4 epoch: 0.6662390015937469\n",
      "Accuracy: 0.8826711250983478\n",
      "New best model saved.\n",
      "Epoch: 5\n",
      "39798\n",
      "Iteration:0/711 loss: 0.5694720149040222\n",
      "Iteration:60/711 loss: 0.5833168029785156\n",
      "Iteration:120/711 loss: 0.5878076553344727\n",
      "Iteration:180/711 loss: 0.6181173324584961\n",
      "Iteration:240/711 loss: 0.5782446265220642\n",
      "Iteration:300/711 loss: 0.6334174275398254\n",
      "Iteration:360/711 loss: 0.620583176612854\n",
      "Iteration:420/711 loss: 0.5735741257667542\n",
      "Iteration:480/711 loss: 0.5531825423240662\n",
      "Iteration:540/711 loss: 0.5873194336891174\n",
      "Iteration:600/711 loss: 0.5692973136901855\n",
      "Iteration:660/711 loss: 0.6419606804847717\n",
      "Validation loss after 5 epoch: 0.6667921277847919\n",
      "Accuracy: 0.8824744295830055\n",
      "Epoch: 6\n",
      "39798\n",
      "Iteration:0/711 loss: 0.587299644947052\n",
      "Iteration:60/711 loss: 0.5704671144485474\n",
      "Iteration:120/711 loss: 0.6236897706985474\n",
      "Iteration:180/711 loss: 0.5693987011909485\n",
      "Iteration:240/711 loss: 0.5527762770652771\n",
      "Iteration:300/711 loss: 0.5700348615646362\n",
      "Iteration:360/711 loss: 0.5810299515724182\n",
      "Iteration:420/711 loss: 0.6052117943763733\n",
      "Iteration:480/711 loss: 0.6052979826927185\n",
      "Iteration:540/711 loss: 0.5696089863777161\n",
      "Iteration:600/711 loss: 0.629623293876648\n",
      "Iteration:660/711 loss: 0.6093436479568481\n",
      "Validation loss after 6 epoch: 0.6656724090104574\n",
      "Accuracy: 0.8816876475216365\n",
      "Epoch: 7\n",
      "39798\n",
      "Iteration:0/711 loss: 0.5895412564277649\n",
      "Iteration:60/711 loss: 0.5514975190162659\n",
      "Iteration:120/711 loss: 0.6408239603042603\n",
      "Iteration:180/711 loss: 0.5871672630310059\n",
      "Iteration:240/711 loss: 0.6062183380126953\n",
      "Iteration:300/711 loss: 0.5888521075248718\n",
      "Iteration:360/711 loss: 0.6019183993339539\n",
      "Iteration:420/711 loss: 0.5772200226783752\n",
      "Iteration:480/711 loss: 0.6054497957229614\n",
      "Iteration:540/711 loss: 0.5700088739395142\n",
      "Iteration:600/711 loss: 0.5531527400016785\n",
      "Iteration:660/711 loss: 0.5866463780403137\n",
      "Validation loss after 7 epoch: 0.6675801840457287\n",
      "Accuracy: 0.8816876475216365\n",
      "Epoch: 8\n",
      "39798\n",
      "Iteration:0/711 loss: 0.5857621431350708\n",
      "Iteration:60/711 loss: 0.55237877368927\n",
      "Iteration:120/711 loss: 0.58713299036026\n",
      "Iteration:180/711 loss: 0.5679029822349548\n",
      "Iteration:240/711 loss: 0.5874238610267639\n",
      "Iteration:300/711 loss: 0.6051214337348938\n",
      "Iteration:360/711 loss: 0.569320797920227\n",
      "Iteration:420/711 loss: 0.5969244241714478\n",
      "Iteration:480/711 loss: 0.5694392919540405\n",
      "Iteration:540/711 loss: 0.5868631601333618\n",
      "Iteration:600/711 loss: 0.5872033834457397\n",
      "Iteration:660/711 loss: 0.6059570908546448\n",
      "Validation loss after 8 epoch: 0.6671386210473029\n",
      "Accuracy: 0.8816876475216365\n",
      "Epoch: 9\n",
      "39798\n",
      "Iteration:0/711 loss: 0.5719357132911682\n",
      "Iteration:60/711 loss: 0.5613163709640503\n",
      "Iteration:120/711 loss: 0.5693224668502808\n",
      "Iteration:180/711 loss: 0.647175133228302\n",
      "Iteration:240/711 loss: 0.6291317343711853\n",
      "Iteration:300/711 loss: 0.5936874747276306\n",
      "Iteration:360/711 loss: 0.5871119499206543\n",
      "Iteration:420/711 loss: 0.5693320035934448\n",
      "Iteration:480/711 loss: 0.5514618754386902\n",
      "Iteration:540/711 loss: 0.6571040749549866\n",
      "Iteration:600/711 loss: 0.5896955728530884\n",
      "Iteration:660/711 loss: 0.6213425993919373\n",
      "Validation loss after 9 epoch: 0.6672553744289901\n",
      "Accuracy: 0.8826711250983478\n",
      "Epoch: 10\n",
      "39798\n",
      "Iteration:0/711 loss: 0.5694537162780762\n",
      "Iteration:60/711 loss: 0.5601066946983337\n",
      "Iteration:120/711 loss: 0.5696515440940857\n",
      "Iteration:180/711 loss: 0.5693073868751526\n",
      "Iteration:240/711 loss: 0.6053782105445862\n",
      "Iteration:300/711 loss: 0.5979812741279602\n",
      "Iteration:360/711 loss: 0.5880545973777771\n",
      "Iteration:420/711 loss: 0.6081618070602417\n",
      "Iteration:480/711 loss: 0.5590984225273132\n",
      "Iteration:540/711 loss: 0.5869752764701843\n",
      "Iteration:600/711 loss: 0.5873953700065613\n",
      "Iteration:660/711 loss: 0.5887221693992615\n",
      "Validation loss after 10 epoch: 0.6628973994936261\n",
      "Accuracy: 0.8857199055861527\n",
      "New best model saved.\n",
      "Epoch: 11\n",
      "39798\n",
      "Iteration:0/711 loss: 0.5695459842681885\n",
      "Iteration:60/711 loss: 0.6577316522598267\n",
      "Iteration:120/711 loss: 0.5538398027420044\n",
      "Iteration:180/711 loss: 0.5580665469169617\n",
      "Iteration:240/711 loss: 0.5871556997299194\n",
      "Iteration:300/711 loss: 0.6023759245872498\n",
      "Iteration:360/711 loss: 0.5812994241714478\n",
      "Iteration:420/711 loss: 0.5802809596061707\n",
      "Iteration:480/711 loss: 0.6517996191978455\n",
      "Iteration:540/711 loss: 0.5693773031234741\n",
      "Iteration:600/711 loss: 0.6136418581008911\n",
      "Iteration:660/711 loss: 0.551649272441864\n",
      "Validation loss after 11 epoch: 0.6641506817969647\n",
      "Accuracy: 0.884441384736428\n",
      "Epoch: 12\n",
      "39798\n",
      "Iteration:0/711 loss: 0.6087278723716736\n",
      "Iteration:60/711 loss: 0.6013241410255432\n",
      "Iteration:120/711 loss: 0.6050390005111694\n",
      "Iteration:180/711 loss: 0.5881373286247253\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 56\n",
    "batches = batchify(df_train, batch_size)\n",
    "length = sum(1 for x in batches)\n",
    "experiment = \"LSTM\"\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    if epoch == 0:\n",
    "        validate(net, df_test, batch_size, epoch)\n",
    "    net.train()\n",
    "    shuffled_df = shuffle_df(df_train)\n",
    "    print(len(shuffled_df))\n",
    "    counter = 0\n",
    "    batches = batchify(shuffled_df, batch_size)\n",
    "    \n",
    "    for df_batch in batches:\n",
    "        \n",
    "        inp = create_input_batch(df_batch)\n",
    "        labels = torch.Tensor(df_batch['y'].values).long()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = net(inp)\n",
    "\n",
    "        batch_loss = criterion(output['out'], labels)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        if(counter % 60 == 0):\n",
    "            print(f\"Iteration:{counter}/{length} loss: {batch_loss.item()}\")\n",
    "            \n",
    "        \n",
    "        counter += 1\n",
    "    _, accuracy = validate(net, df_test, batch_size, epoch)\n",
    "    saved = save_best_model(net, experiment, accuracy)\n",
    "    if saved:\n",
    "        print(f\"New best model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('app/lang_model/data/vocab/vocab.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(vocab, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
